# 第二课笔记

## 图像表达形式 
rgb 3通道
图像是像素构成的数组：𝑋 ∈ ℝ𝐻×𝑊×3
对类别进行编号：香蕉→1，苹果→2，橘子→3 等等，得到类别 𝑦 ∈ {1, … ,𝐾}，𝐾 为类别总数
图像分类问题：构建一个可计算实现的函数 𝐹: ℝ𝐻×𝑊×3 → {1, … ,𝐾}，且预测结果符合人类认知

## cnn
卷积网络，通过卷积核抽取特征， 进行数据分类

### 图像分类的数学表示
1. 模型设计：设计适合图像的 𝐹Θ 𝑋 
• 卷积神经网络
• 轻量化卷积神经网络
• 神经结构搜索
• Transformer
2. 模型学习：求解一组好的参数 Θ 
• 监督学习：基于标注数据学习
• 损失函数
• 随机梯度下降算法
• 视觉模型常用训练技巧
• 自监督学习：基于无标注的数据学习


### 经典网络
AlexNet
• 第一个成功实现大规模图像的模型，在 ImageNet 数据集上达到 ~85% 的 top-5 准确率
• 5 个卷积层，3 个全连接层，共有 60M 个可学习参数
• 使用 ReLU 激活函数，大幅提高收敛速度
• 实现并开源了 cuda-convnet ，在 GPU 上训练大规模神经网络在工程上成为可能

GoogLeNet
• 使用 Inception 模块堆叠形成， 22 个可学习层
• 最后的分类仅使用单层全连接层，可节省大量参数
• 仅 7M 权重参数（AlexNet 60M、VGG 138M）

问题
精度退化问题
模型层数增加到一定程度后，分类正确率不增反降
卷积退化为恒等映射时，深层网络与浅层网络相同
所以，深层网络应具备不差于浅层网络的分类精度

Resnet
残差学习，通过引入残差建模,让新增加的层拟合浅层网络与深层网络之间的差异，更容易学习
梯度可以直接回传到浅层网络监督浅层网络的学习
没有引入额外参入，让参数更有效贡献到最终的模型中


Transformer
使用 Transformer 替代卷积网络实现图像分类，使用更大的数据集训练，达到超越卷积网络的精度